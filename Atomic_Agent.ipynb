{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO12FORVhHIBmTvjVzJbY4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gzmlab/Building-Agentic-AI-Systems/blob/main/Atomic_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##[Don’t use Langchain anymore : Atomic Agents is the new paradigm !](https://data-ai.theodo.com/en/technical-blog/dont-use-langchain-anymore-use-atomic-agents)"
      ],
      "metadata": {
        "id": "0zouK8DI3JAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install atomic-agents openai -q"
      ],
      "metadata": {
        "id": "cJnNe0gE2UFc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HDVQm3L82Pri"
      },
      "outputs": [],
      "source": [
        "from atomic_agents.agents.base_agent import BaseIOSchema\n",
        "from pydantic import Field\n",
        "\n",
        "class AnswerAgentInputSchema(BaseIOSchema):\n",
        "    \"\"\"Input schema for the AnswerAgent.\"\"\"\n",
        "    question: str = Field(..., description=\"A question that needs to be answered based on the provided context. Also output a confidence score.\",)\n",
        "\n",
        "class AnswerAgentOutputSchema(BaseIOSchema):\n",
        "    \"\"\"Output schema for the AnswerAgent.\"\"\"\n",
        "    text_output: str = Field(..., description=\"The answer to the question in markdown format.\")\n",
        "    confidence_score: str = Field(..., description=\"The confidence score about your answer\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n",
        "\n",
        "system_prompt_generator=SystemPromptGenerator(\n",
        "    background=[\n",
        "         \"You are an intelligent answering expert.\",\n",
        "         \"Your task is to provide accurate and detailed answers to user questions basedoon the given context.\"],\n",
        "    steps=[\n",
        "          \"You will receive a question and the context information.\",\n",
        "          \"Generate a detailed and accurate answer based on the context.\",\n",
        "    ],\n",
        "    output_instructions=[\n",
        "        \"Ensure clarity and conciseness in each answer.\",\n",
        "        \"Ensure the answer is directly relevant to the question and context provided.\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "l81aByxN3XKZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import instructor\n",
        "import openai\n",
        "from typing import List\n",
        "from atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig\n",
        "from atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "LLM_MODEL_NAME = \"gpt-4o-mini\" # Define the model name\n",
        "\n",
        "answer_agent = BaseAgent(\n",
        "     BaseAgentConfig(\n",
        "         client=instructor.from_openai(openai.OpenAI(api_key=api_key)), # Pass the API key here\n",
        "         model=LLM_MODEL_NAME,\n",
        "         system_prompt_generator=system_prompt_generator,\n",
        "         input_schema=AnswerAgentInputSchema,\n",
        "         output_schema=AnswerAgentOutputSchema,\n",
        "         memory=None, # Memory is an object. Memory can be shared accros many agents !\n",
        "         temperature=0,\n",
        "         max_tokens=None, # Constraints the number of output tokens\n",
        "     ) )"
      ],
      "metadata": {
        "id": "pfr4Tbvw3i_6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import instructor\n",
        "import openai\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.text import Text\n",
        "from typing import List\n",
        "from pydantic import Field\n",
        "from atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n",
        "from atomic_agents.lib.components.agent_memory import AgentMemory\n",
        "from atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseAgentInputSchema\n",
        "from atomic_agents.lib.base.base_io_schema import BaseIOSchema\n",
        "\n",
        "# API Key setup\n",
        "API_KEY = \"\"\n",
        "if not API_KEY:\n",
        "    API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\n",
        "        \"API key is not set. Please set the API key as a static variable or in the environment variable OPENAI_API_KEY.\"\n",
        "    )\n",
        "\n",
        "# Initialize a Rich Console for pretty console outputs\n",
        "console = Console()\n",
        "\n",
        "# Memory setup\n",
        "memory = AgentMemory()\n",
        "\n",
        "\n",
        "# Custom output schema\n",
        "class CustomOutputSchema(BaseIOSchema):\n",
        "    \"\"\"This schema represents the response generated by the chat agent, including suggested follow-up questions.\"\"\"\n",
        "\n",
        "    chat_message: str = Field(\n",
        "        ...,\n",
        "        description=\"The chat message exchanged between the user and the chat agent.\",\n",
        "    )\n",
        "    suggested_user_questions: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"A list of suggested follow-up questions the user could ask the agent.\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Initialize memory with an initial message from the assistant\n",
        "initial_message = CustomOutputSchema(\n",
        "    chat_message=\"Hello! How can I assist you today?\",\n",
        "    suggested_user_questions=[\"What can you do?\", \"Tell me a joke\", \"Tell me about how you were made\"],\n",
        ")\n",
        "memory.add_message(\"assistant\", initial_message)\n",
        "\n",
        "# OpenAI client setup using the Instructor library\n",
        "client = instructor.from_openai(openai.OpenAI(api_key=API_KEY))\n",
        "\n",
        "# Custom system prompt\n",
        "system_prompt_generator = SystemPromptGenerator(\n",
        "    background=[\n",
        "        \"This assistant is a knowledgeable AI designed to be helpful, friendly, and informative.\",\n",
        "        \"It has a wide range of knowledge on various topics and can engage in diverse conversations.\",\n",
        "    ],\n",
        "    steps=[\n",
        "        \"Analyze the user's input to understand the context and intent.\",\n",
        "        \"Formulate a relevant and informative response based on the assistant's knowledge.\",\n",
        "        \"Generate 3 suggested follow-up questions for the user to explore the topic further.\",\n",
        "    ],\n",
        "    output_instructions=[\n",
        "        \"Provide clear, concise, and accurate information in response to user queries.\",\n",
        "        \"Maintain a friendly and professional tone throughout the conversation.\",\n",
        "        \"Conclude each response with 3 relevant suggested questions for the user.\",\n",
        "    ],\n",
        ")\n",
        "console.print(Panel(system_prompt_generator.generate_prompt(), width=console.width, style=\"bold cyan\"), style=\"bold cyan\")\n",
        "\n",
        "# Agent setup with specified configuration and custom output schema\n",
        "agent = BaseAgent(\n",
        "    config=BaseAgentConfig(\n",
        "        client=client,\n",
        "        model=\"gpt-4o-mini\",\n",
        "        system_prompt_generator=system_prompt_generator,\n",
        "        memory=memory,\n",
        "        output_schema=CustomOutputSchema,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the initial message from the assistant\n",
        "console.print(Text(\"Agent:\", style=\"bold green\"), end=\" \")\n",
        "console.print(Text(initial_message.chat_message, style=\"bold green\"))\n",
        "\n",
        "# Display initial suggested questions\n",
        "console.print(\"\\n[bold cyan]Suggested questions you could ask:[/bold cyan]\")\n",
        "for i, question in enumerate(initial_message.suggested_user_questions, 1):\n",
        "    console.print(f\"[cyan]{i}. {question}[/cyan]\")\n",
        "console.print()  # Add an empty line for better readability\n",
        "\n",
        "# Start an infinite loop to handle user inputs and agent responses\n",
        "while True:\n",
        "    # Prompt the user for input with a styled prompt\n",
        "    user_input = console.input(\"[bold blue]You:[/bold blue] \")\n",
        "    # Check if the user wants to exit the chat\n",
        "    if user_input.lower() in [\"/exit\", \"/quit\"]:\n",
        "        console.print(\"Exiting chat...\")\n",
        "        break\n",
        "\n",
        "    # Process the user's input through the agent and get the response\n",
        "    response = agent.run(BaseAgentInputSchema(chat_message=user_input))\n",
        "\n",
        "    # Display the agent's response\n",
        "    agent_message = Text(response.chat_message, style=\"bold green\")\n",
        "    console.print(Text(\"Agent:\", style=\"bold green\"), end=\" \")\n",
        "    console.print(agent_message)\n",
        "\n",
        "    # Display follow-up questions\n",
        "    console.print(\"\\n[bold cyan]Suggested questions you could ask:[/bold cyan]\")\n",
        "    for i, question in enumerate(response.suggested_user_questions, 1):\n",
        "        console.print(f\"[cyan]{i}. {question}[/cyan]\")\n",
        "    console.print()  # Add an empty line for better readability"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WEF2ipt44JGb",
        "outputId": "3ffcabcb-e747-4532-ffd2-537d6c5326d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m# IDENTITY and PURPOSE\u001b[0m\u001b[1;36m                                                                                         \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- This assistant is a knowledgeable AI designed to be helpful, friendly, and informative.\u001b[0m\u001b[1;36m                      \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- It has a wide range of knowledge on various topics and can engage in diverse conversations.\u001b[0m\u001b[1;36m                  \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m                                                                                                               \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m# INTERNAL ASSISTANT STEPS\u001b[0m\u001b[1;36m                                                                                     \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Analyze the user's input to understand the context and intent.\u001b[0m\u001b[1;36m                                               \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Formulate a relevant and informative response based on the assistant's knowledge.\u001b[0m\u001b[1;36m                            \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Generate 3 suggested follow-up questions for the user to explore the topic further.\u001b[0m\u001b[1;36m                          \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m                                                                                                               \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m# OUTPUT INSTRUCTIONS\u001b[0m\u001b[1;36m                                                                                          \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Provide clear, concise, and accurate information in response to user queries.\u001b[0m\u001b[1;36m                                \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Maintain a friendly and professional tone throughout the conversation.\u001b[0m\u001b[1;36m                                       \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Conclude each response with 3 relevant suggested questions for the user.\u001b[0m\u001b[1;36m                                     \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Always respond using the proper JSON schema.\u001b[0m\u001b[1;36m                                                                 \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m│\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m- Always use the available additional information and context to enhance the response.\u001b[0m\u001b[1;36m                         \u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m│\u001b[0m\n",
              "\u001b[1;36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ # IDENTITY and PURPOSE                                                                                          │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - This assistant is a knowledgeable AI designed to be helpful, friendly, and informative.                       │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - It has a wide range of knowledge on various topics and can engage in diverse conversations.                   │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│                                                                                                                 │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ # INTERNAL ASSISTANT STEPS                                                                                      │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Analyze the user's input to understand the context and intent.                                                │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Formulate a relevant and informative response based on the assistant's knowledge.                             │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Generate 3 suggested follow-up questions for the user to explore the topic further.                           │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│                                                                                                                 │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ # OUTPUT INSTRUCTIONS                                                                                           │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Provide clear, concise, and accurate information in response to user queries.                                 │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Maintain a friendly and professional tone throughout the conversation.                                        │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Conclude each response with 3 relevant suggested questions for the user.                                      │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Always respond using the proper JSON schema.                                                                  │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">│ - Always use the available additional information and context to enhance the response.                          │</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mHello! How can I assist you today?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Hello! How can I assist you today?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;36mSuggested questions you could ask:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Suggested questions you could ask:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m\u001b[36m. What can you do?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. What can you do?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m\u001b[36m. Tell me a joke\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. Tell me a joke</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m3\u001b[0m\u001b[36m. Tell me about how you were made\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. Tell me about how you were made</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mYou:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">You:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mI can do a variety of things, including answering questions on a wide range of topics, providing information, \u001b[0m\n",
              "\u001b[1;32mengaging in conversation, and offering assistance with problem-solving. Whether you're curious about science, \u001b[0m\n",
              "\u001b[1;32mhistory, technology, or just want to chat, I'm here to help!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">I can do a variety of things, including answering questions on a wide range of topics, providing information, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">engaging in conversation, and offering assistance with problem-solving. Whether you're curious about science, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">history, technology, or just want to chat, I'm here to help!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;36mSuggested questions you could ask:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Suggested questions you could ask:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m\u001b[36m. Can you answer specific questions about a topic?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. Can you answer specific questions about a topic?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m\u001b[36m. What kind of information do you have?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. What kind of information do you have?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m3\u001b[0m\u001b[36m. How do you learn new things?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. How do you learn new things?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mYou:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">You:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mI learn from a vast dataset that includes a diverse range of information and knowledge up to October 2023. This \u001b[0m\n",
              "\u001b[1;32mdataset is built from books, articles, websites, and other texts, which allows me to generate responses and provide\u001b[0m\n",
              "\u001b[1;32minsights based on that foundational knowledge. However, I do not learn in real time or update myself with new \u001b[0m\n",
              "\u001b[1;32minformation after that date.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">I learn from a vast dataset that includes a diverse range of information and knowledge up to October 2023. This </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">dataset is built from books, articles, websites, and other texts, which allows me to generate responses and provide</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">insights based on that foundational knowledge. However, I do not learn in real time or update myself with new </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">information after that date.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;36mSuggested questions you could ask:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Suggested questions you could ask:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m\u001b[36m. What subjects do you know best?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">. What subjects do you know best?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m\u001b[36m. Can you provide information on recent events?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">. Can you provide information on recent events?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m3\u001b[0m\u001b[36m. How can I find out more about a specific topic?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\">. How can I find out more about a specific topic?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mYou:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">You:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-3083397016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Prompt the user for input with a styled prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[bold blue]You:[/bold blue] \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Check if the user wants to exit the chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"/exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(self, prompt, markup, emoji, password, stream)\u001b[0m\n\u001b[1;32m   2149\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}